{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#J_Learn_ML_PyTorch_mnist_02_mlp\n",
    "# Fit MLP on mnist\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "\n",
    "from IPython.core.debugger import Pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data, and sampling mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Dataset\n",
    "trainset=torchvision.datasets.MNIST('/tmp',train=True,download=True,\n",
    "                                    transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# 1.3 train-validation partition\n",
    "tvRatio=0.8\n",
    "batchSz=128\n",
    "\n",
    "nSample=len(trainset)\n",
    "nTrain=int(nSample*tvRatio)\n",
    "idxPerm=np.random.permutation(nSample)\n",
    "idx_t=idxPerm[:nTrain]\n",
    "idx_v=idxPerm[nTrain:]\n",
    "\n",
    "sampler_t=torch.utils.data.sampler.SubsetRandomSampler(idx_t)\n",
    "sampler_v=torch.utils.data.sampler.SubsetRandomSampler(idx_v)\n",
    "\n",
    "loader_t=torch.utils.data.DataLoader(dataset=trainset,batch_size=batchSz,sampler=sampler_t,\n",
    "                                     drop_last=True)\n",
    "loader_v=torch.utils.data.DataLoader(dataset=trainset,batch_size=batchSz,sampler=sampler_v,\n",
    "                                     drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specification of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self,nFeat,nClass):\n",
    "        super(MLP,self).__init__()\n",
    "        self.fc_1=torch.nn.Linear(nFeat,300)\n",
    "        self.relu_1=torch.nn.ReLU()\n",
    "        self.fc_o=torch.nn.Linear(300,nClass)\n",
    "        self.out=torch.nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"x should be [N x nFeature]\"\"\"\n",
    "        x=self.fc_1(x)\n",
    "        x=self.relu_1(x)\n",
    "        x=self.fc_o(x)\n",
    "        x=self.out(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# GPU\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "The major steps of training is to\n",
    "  - Compute the predicated value from the neural network, by calling iutput=net(input)\n",
    "      - zero out all grad before calling net(), using optimizer.zero_grad()\n",
    "  - Compute loss, by calling loss=loss_function(output, input)\n",
    "  - Obtain the derivatives of loss w.r.t. each parameter, by calling loss.backward()\n",
    "  - Modify the parameters as specified by optimizer, calling optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc_1): Linear(in_features=784, out_features=300, bias=True)\n",
      "  (relu_1): ReLU()\n",
      "  (fc_o): Linear(in_features=300, out_features=10, bias=True)\n",
      "  (out): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters -------\n",
    "lhyper={'nEpoch':100, 'early':15, 'batchSz':batchSz,\n",
    "        'lr':0.001, 'lrReduce':0.5, 'lrPatience':6,\n",
    "        'pDrop':0.0, 'weight_decay':0.5}\n",
    "\n",
    "# Neural network --------\n",
    "# Feature dimensions\n",
    "img,_=next(iter(loader_t))\n",
    "nFeat=img.size(1)*img.size(2)*img.size(3)\n",
    "nClass=10 # digits\n",
    "\n",
    "net=MLP(nFeat,nClass).to(device)\n",
    "print(net)\n",
    "\n",
    "# Loss, optimizer -------\n",
    "#metricF=torch.nn.NLLLoss(reduction='elementwise_mean')  # **look at .grad_fn attribute**\n",
    "metricF=torch.nn.NLLLoss(reduction='none')  # **look at .grad_fn attribute**\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lhyper['lr'],\n",
    "                           weight_decay=lhyper['weight_decay'])\n",
    "\n",
    "# Learning rate scheduler ------\n",
    "lrScheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',\n",
    "                                                       factor=lhyper['lrReduce'],\n",
    "                                                       patience=lhyper['lrPatience'],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100: loss train=0.4283, val=0.2837\n",
      "  bestEpoch=0, val=0.2837\n",
      "1/100: loss train=0.2461, val=0.2241\n",
      "  bestEpoch=1, val=0.2241\n",
      "2/100: loss train=0.2115, val=0.1960\n",
      "  bestEpoch=2, val=0.1960\n",
      "3/100: loss train=0.1951, val=0.1983\n",
      "  bestEpoch=2, val=0.1960\n",
      "4/100: loss train=0.1855, val=0.1744\n",
      "  bestEpoch=4, val=0.1744\n",
      "5/100: loss train=0.1780, val=0.1738\n",
      "  bestEpoch=5, val=0.1738\n",
      "6/100: loss train=0.1730, val=0.1726\n",
      "  bestEpoch=6, val=0.1726\n",
      "7/100: loss train=0.1717, val=0.1682\n",
      "  bestEpoch=7, val=0.1682\n",
      "8/100: loss train=0.1701, val=0.1638\n",
      "  bestEpoch=8, val=0.1638\n",
      "9/100: loss train=0.1683, val=0.1767\n",
      "  bestEpoch=8, val=0.1638\n",
      "10/100: loss train=0.1683, val=0.1667\n",
      "  bestEpoch=8, val=0.1638\n",
      "11/100: loss train=0.1673, val=0.1700\n",
      "  bestEpoch=8, val=0.1638\n",
      "12/100: loss train=0.1646, val=0.1695\n",
      "  bestEpoch=8, val=0.1638\n",
      "13/100: loss train=0.1659, val=0.1617\n",
      "  bestEpoch=13, val=0.1617\n",
      "14/100: loss train=0.1648, val=0.1693\n",
      "  bestEpoch=13, val=0.1617\n",
      "15/100: loss train=0.1641, val=0.1614\n",
      "  bestEpoch=15, val=0.1614\n",
      "16/100: loss train=0.1638, val=0.1668\n",
      "  bestEpoch=15, val=0.1614\n",
      "17/100: loss train=0.1618, val=0.1659\n",
      "  bestEpoch=15, val=0.1614\n",
      "18/100: loss train=0.1618, val=0.1628\n",
      "  bestEpoch=15, val=0.1614\n",
      "19/100: loss train=0.1627, val=0.1602\n",
      "  bestEpoch=19, val=0.1602\n",
      "20/100: loss train=0.1620, val=0.1653\n",
      "  bestEpoch=19, val=0.1602\n",
      "21/100: loss train=0.1605, val=0.1633\n",
      "  bestEpoch=19, val=0.1602\n",
      "22/100: loss train=0.1606, val=0.1647\n",
      "  bestEpoch=19, val=0.1602\n",
      "23/100: loss train=0.1609, val=0.1726\n",
      "  bestEpoch=19, val=0.1602\n",
      "24/100: loss train=0.1617, val=0.1623\n",
      "  bestEpoch=19, val=0.1602\n",
      "25/100: loss train=0.1603, val=0.1623\n",
      "  bestEpoch=19, val=0.1602\n",
      "26/100: loss train=0.1596, val=0.1604\n",
      "  bestEpoch=19, val=0.1602\n",
      "Epoch    26: reducing learning rate of group 0 to 5.0000e-04.\n",
      "27/100: loss train=0.1524, val=0.1575\n",
      "  bestEpoch=27, val=0.1575\n",
      "28/100: loss train=0.1533, val=0.1574\n",
      "  bestEpoch=28, val=0.1574\n",
      "29/100: loss train=0.1522, val=0.1582\n",
      "  bestEpoch=28, val=0.1574\n",
      "30/100: loss train=0.1523, val=0.1569\n",
      "  bestEpoch=30, val=0.1569\n",
      "31/100: loss train=0.1529, val=0.1575\n",
      "  bestEpoch=30, val=0.1569\n",
      "32/100: loss train=0.1522, val=0.1593\n",
      "  bestEpoch=30, val=0.1569\n",
      "33/100: loss train=0.1521, val=0.1581\n",
      "  bestEpoch=30, val=0.1569\n",
      "34/100: loss train=0.1521, val=0.1609\n",
      "  bestEpoch=30, val=0.1569\n",
      "35/100: loss train=0.1525, val=0.1555\n",
      "  bestEpoch=35, val=0.1555\n",
      "36/100: loss train=0.1519, val=0.1594\n",
      "  bestEpoch=35, val=0.1555\n",
      "37/100: loss train=0.1519, val=0.1562\n",
      "  bestEpoch=35, val=0.1555\n",
      "38/100: loss train=0.1517, val=0.1568\n",
      "  bestEpoch=35, val=0.1555\n",
      "39/100: loss train=0.1520, val=0.1560\n",
      "  bestEpoch=35, val=0.1555\n",
      "40/100: loss train=0.1513, val=0.1568\n",
      "  bestEpoch=35, val=0.1555\n",
      "41/100: loss train=0.1521, val=0.1563\n",
      "  bestEpoch=35, val=0.1555\n",
      "42/100: loss train=0.1517, val=0.1572\n",
      "  bestEpoch=35, val=0.1555\n",
      "Epoch    42: reducing learning rate of group 0 to 2.5000e-04.\n",
      "43/100: loss train=0.1473, val=0.1548\n",
      "  bestEpoch=43, val=0.1548\n",
      "44/100: loss train=0.1475, val=0.1564\n",
      "  bestEpoch=43, val=0.1548\n",
      "45/100: loss train=0.1476, val=0.1542\n",
      "  bestEpoch=45, val=0.1542\n",
      "46/100: loss train=0.1476, val=0.1535\n",
      "  bestEpoch=46, val=0.1535\n",
      "47/100: loss train=0.1475, val=0.1554\n",
      "  bestEpoch=46, val=0.1535\n",
      "48/100: loss train=0.1473, val=0.1553\n",
      "  bestEpoch=46, val=0.1535\n",
      "49/100: loss train=0.1474, val=0.1562\n",
      "  bestEpoch=46, val=0.1535\n",
      "50/100: loss train=0.1476, val=0.1544\n",
      "  bestEpoch=46, val=0.1535\n",
      "51/100: loss train=0.1475, val=0.1541\n",
      "  bestEpoch=46, val=0.1535\n",
      "52/100: loss train=0.1475, val=0.1544\n",
      "  bestEpoch=46, val=0.1535\n",
      "53/100: loss train=0.1473, val=0.1555\n",
      "  bestEpoch=46, val=0.1535\n",
      "Epoch    53: reducing learning rate of group 0 to 1.2500e-04.\n",
      "54/100: loss train=0.1450, val=0.1537\n",
      "  bestEpoch=46, val=0.1535\n",
      "55/100: loss train=0.1451, val=0.1537\n",
      "  bestEpoch=46, val=0.1535\n",
      "56/100: loss train=0.1450, val=0.1538\n",
      "  bestEpoch=46, val=0.1535\n",
      "57/100: loss train=0.1451, val=0.1541\n",
      "  bestEpoch=46, val=0.1535\n",
      "58/100: loss train=0.1450, val=0.1538\n",
      "  bestEpoch=46, val=0.1535\n",
      "59/100: loss train=0.1450, val=0.1534\n",
      "  bestEpoch=59, val=0.1534\n",
      "60/100: loss train=0.1449, val=0.1536\n",
      "  bestEpoch=59, val=0.1534\n",
      "61/100: loss train=0.1451, val=0.1537\n",
      "  bestEpoch=59, val=0.1534\n",
      "62/100: loss train=0.1450, val=0.1541\n",
      "  bestEpoch=59, val=0.1534\n",
      "63/100: loss train=0.1454, val=0.1529\n",
      "  bestEpoch=63, val=0.1529\n",
      "64/100: loss train=0.1449, val=0.1542\n",
      "  bestEpoch=63, val=0.1529\n",
      "65/100: loss train=0.1449, val=0.1537\n",
      "  bestEpoch=63, val=0.1529\n",
      "66/100: loss train=0.1450, val=0.1538\n",
      "  bestEpoch=63, val=0.1529\n",
      "67/100: loss train=0.1449, val=0.1538\n",
      "  bestEpoch=63, val=0.1529\n",
      "68/100: loss train=0.1448, val=0.1534\n",
      "  bestEpoch=63, val=0.1529\n",
      "69/100: loss train=0.1448, val=0.1529\n",
      "  bestEpoch=69, val=0.1529\n",
      "70/100: loss train=0.1449, val=0.1532\n",
      "  bestEpoch=69, val=0.1529\n",
      "71/100: loss train=0.1449, val=0.1540\n",
      "  bestEpoch=69, val=0.1529\n",
      "72/100: loss train=0.1445, val=0.1529\n",
      "  bestEpoch=69, val=0.1529\n",
      "73/100: loss train=0.1448, val=0.1544\n",
      "  bestEpoch=69, val=0.1529\n",
      "74/100: loss train=0.1449, val=0.1536\n",
      "  bestEpoch=69, val=0.1529\n",
      "75/100: loss train=0.1448, val=0.1541\n",
      "  bestEpoch=69, val=0.1529\n",
      "76/100: loss train=0.1448, val=0.1529\n",
      "  bestEpoch=69, val=0.1529\n",
      "Epoch    76: reducing learning rate of group 0 to 6.2500e-05.\n",
      "77/100: loss train=0.1435, val=0.1534\n",
      "  bestEpoch=69, val=0.1529\n",
      "78/100: loss train=0.1434, val=0.1528\n",
      "  bestEpoch=78, val=0.1528\n",
      "79/100: loss train=0.1433, val=0.1527\n",
      "  bestEpoch=79, val=0.1527\n",
      "80/100: loss train=0.1434, val=0.1533\n",
      "  bestEpoch=79, val=0.1527\n",
      "81/100: loss train=0.1433, val=0.1528\n",
      "  bestEpoch=79, val=0.1527\n",
      "82/100: loss train=0.1435, val=0.1530\n",
      "  bestEpoch=79, val=0.1527\n",
      "83/100: loss train=0.1435, val=0.1528\n",
      "  bestEpoch=79, val=0.1527\n",
      "84/100: loss train=0.1434, val=0.1534\n",
      "  bestEpoch=79, val=0.1527\n",
      "85/100: loss train=0.1435, val=0.1528\n",
      "  bestEpoch=79, val=0.1527\n",
      "86/100: loss train=0.1434, val=0.1540\n",
      "  bestEpoch=79, val=0.1527\n",
      "Epoch    86: reducing learning rate of group 0 to 3.1250e-05.\n",
      "87/100: loss train=0.1427, val=0.1529\n",
      "  bestEpoch=79, val=0.1527\n",
      "88/100: loss train=0.1426, val=0.1529\n",
      "  bestEpoch=79, val=0.1527\n",
      "89/100: loss train=0.1426, val=0.1527\n",
      "  bestEpoch=89, val=0.1527\n",
      "90/100: loss train=0.1427, val=0.1529\n",
      "  bestEpoch=89, val=0.1527\n",
      "91/100: loss train=0.1426, val=0.1532\n",
      "  bestEpoch=89, val=0.1527\n",
      "92/100: loss train=0.1426, val=0.1529\n",
      "  bestEpoch=89, val=0.1527\n",
      "93/100: loss train=0.1426, val=0.1531\n",
      "  bestEpoch=89, val=0.1527\n",
      "94/100: loss train=0.1426, val=0.1528\n",
      "  bestEpoch=89, val=0.1527\n",
      "95/100: loss train=0.1426, val=0.1528\n",
      "  bestEpoch=89, val=0.1527\n",
      "96/100: loss train=0.1427, val=0.1530\n",
      "  bestEpoch=89, val=0.1527\n",
      "Epoch    96: reducing learning rate of group 0 to 1.5625e-05.\n",
      "97/100: loss train=0.1422, val=0.1528\n",
      "  bestEpoch=89, val=0.1527\n",
      "98/100: loss train=0.1422, val=0.1528\n",
      "  bestEpoch=89, val=0.1527\n",
      "99/100: loss train=0.1422, val=0.1531\n",
      "  bestEpoch=89, val=0.1527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'epoch')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7504b63c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lossC={'t':np.zeros((lhyper['nEpoch'],),dtype=np.float32),\n",
    "       'v':np.zeros((lhyper['nEpoch'],),dtype=np.float32)}\n",
    "bestLoss={'t':np.inf, 'v':np.inf, 'epoch':-1}\n",
    "bestWeight=None\n",
    "\n",
    "# Training loop ------\n",
    "for iEpoch in range(lhyper['nEpoch']):\n",
    "    # Train\n",
    "    net.train(mode=True)\n",
    "    for iBatch, (img, label) in enumerate(loader_t):\n",
    "        optimizer.zero_grad() # Understand why *****\n",
    "        curYp=net(img.view(-1,nFeat).to(device))\n",
    "        #loss=metricF(curYp,label.to(device)) # with 'elementwise_mean'\n",
    "        loss=torch.sum(metricF(curYp,label.to(device))) # with reduction='none'\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lossC['t'][iEpoch]+=loss.item()\n",
    "    lossC['t'][iEpoch] /= (iBatch+1)*batchSz # Training discarded incomplete batch\n",
    "    #lossC['t'][iEpoch] /= (iBatch+1) # with 'elementwise_mean'\n",
    "        \n",
    "    # Validation\n",
    "    net.train(mode=False)\n",
    "    for iBatch, (img, label) in enumerate(loader_v):\n",
    "        curYp=net(img.view(-1,nFeat).to(device))\n",
    "        #loss=metricF(curYp,label.to(device)) # with 'elementwise_mean'\n",
    "        loss=torch.sum(metricF(curYp,label.to(device)))\n",
    "        lossC['v'][iEpoch]+=loss.item()\n",
    "    lossC['v'][iEpoch] /= idx_v.shape[0] # Validation uses all data\n",
    "    #lossC['v'][iEpoch] /= (iBatch+1) # with 'elementwise_mean'\n",
    "    \n",
    "    # End of epoch processing: lr adjustment, early stop, loop output\n",
    "    if lossC['v'][iEpoch]<bestLoss['v']:\n",
    "        bestLoss.update({'t':lossC['t'][iEpoch], 'v':lossC['v'][iEpoch], 'epoch':iEpoch})\n",
    "        bestWeight=copy.deepcopy(net.state_dict())\n",
    "    print('{:d}/{:d}: loss train={:.4f}, val={:.4f}'.format(iEpoch,lhyper['nEpoch'],lossC['t'][iEpoch],\n",
    "                                                            lossC['v'][iEpoch]))\n",
    "    print('  bestEpoch={:d}, val={:.4f}'.format(bestLoss['epoch'],bestLoss['v']))\n",
    "    if iEpoch>bestLoss['epoch']+lhyper['early']:\n",
    "        break\n",
    "    lrScheduler.step(lossC['v'][iEpoch])\n",
    "lastEpoch=iEpoch\n",
    "\n",
    "# Learning curve -----\n",
    "plt.figure(1,figsize=(4,3))\n",
    "plt.clf()\n",
    "_,ax=plt.subplots(nrows=1,ncols=1,num=1)\n",
    "ax.plot(lossC['t'][:lastEpoch],'-',color='b',label='Train')\n",
    "ax.plot(lossC['v'][:lastEpoch],'-',color=[1,.5,0],label='Val')\n",
    "ax.plot(bestLoss['epoch'],bestLoss['v'],'x',color='k',label='best [{:.4f}]'.format(bestLoss['v']))\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict(model,loader,nFeat):\n",
    "    \"\"\"Predict\n",
    "    \n",
    "    Need nFeat to convert image (2D) to vector\n",
    "    \"\"\"\n",
    "    pv=[]\n",
    "    label=[]\n",
    "    loss=0\n",
    "    for iBatch, (img, label_) in enumerate(loader):\n",
    "        label.append(label_.numpy())\n",
    "        curYp=net(img.view(-1,nFeat).to(model['device']))\n",
    "        loss+=torch.sum(model['metric'](curYp,label_.to(model['device']))).data.item()\n",
    "        pv.append(curYp.to('cpu').detach().numpy())\n",
    "    pv=np.concatenate(pv,axis=0)\n",
    "    label=np.concatenate(label,axis=0)\n",
    "    return pv,label,loss\n",
    "\n",
    "# Best net\n",
    "bestNet=MLP(nFeat,nClass)\n",
    "bestNet.load_state_dict(bestWeight)\n",
    "bestNet.to(device)\n",
    "bestNet.train(mode=False)\n",
    "\n",
    "model={'net':bestNet, 'metric':metricF, 'device':device}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 46476/48000 = 0.9683\n",
      "Validation accuracy: 11569/12000 = 0.9641\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "pv_t,label_t,loss_t=predict(model,loader_t,nFeat)\n",
    "pred_t=np.argmax(pv_t,axis=1)\n",
    "print('Train accuracy: {:d}/{:d} = {:.4f}'.format(np.sum(pred_t==label_t),label_t.shape[0],\n",
    "                                                  np.sum(pred_t==label_t)/label_t.shape[0]) )\n",
    "\n",
    "# Validation\n",
    "pv_v,label_v,loss_v=predict(model,loader_v,nFeat)\n",
    "pred_v=np.argmax(pv_v,axis=1)\n",
    "print('Validation accuracy: {:d}/{:d} = {:.4f}'.format(np.sum(pred_v==label_v),label_v.shape[0],\n",
    "                                                       np.sum(pred_v==label_v)/label_v.shape[0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 9657/10000 = 0.9657\n"
     ]
    }
   ],
   "source": [
    "# compare results with http://yann.lecun.com/exdb/mnist/\n",
    "checkset=torchvision.datasets.MNIST('/tmp',train=False,download=True,\n",
    "                                    transform=torchvision.transforms.ToTensor())\n",
    "loader_c=torch.utils.data.DataLoader(checkset,batch_size=batchSz,shuffle=False)\n",
    "pv_c,label_c,loss_c=predict(model,loader_c,nFeat)\n",
    "pred_c=np.argmax(pv_c,axis=1)\n",
    "print('Test accuracy: 1-{:d}/{:d} = {:.4f}'.format(np.sum(pred_c==label_c),label_c.shape[0],\n",
    "                                                   1-np.sum(pred_c==label_c)/label_c.shape[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(checkset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next step\n",
    "\n",
    "  - Autoencode and autodecoder, compare with PCA for e.g. 2 dimension code\n",
    "  - Convolution neural network\n",
    "      - Visualizing the feature extraction and other ways to understand the neural network behavior\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
